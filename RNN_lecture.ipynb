{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_lecture.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNlrVhi/44lP3Bkw30UPCLW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimisz/tensorflow_lectures/blob/main/RNN_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX0RjgVDYjN1"
      },
      "source": [
        "**Recurrent Neural Networks** are more effective with sequence data, e.g.\n",
        "* time-stamped sales data\n",
        "* sequence of text\n",
        "* heart beat data\n",
        "* etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSiHKqTqbYry"
      },
      "source": [
        "## RNN Theory\n",
        "**Sequence** is data that's coming in a specific order, and order is important.\n",
        "\n",
        "**Normal Neuron workflow**:\n",
        "Input -> Aggregation of Inputs -> Activation Function -> Output\n",
        "\n",
        "**Recurrent Neuron workflow**: Input -> Aggregation of Inputs -> Activation Function -> Output -> Aggrregation of Inputs (output is fed to itself)\n",
        "\n",
        "**Memory Cells** - cells that are a function of inputs from previous time steps.\n",
        "\n",
        "**RNN Architectures**:\n",
        "* **Sequence-to-Sequence** (Many to Many): *pass in a sequence and expect a sequence out (e.g. chatbot)*\n",
        "* **Sequence-to-Vector** (Many to One): *pass in a sequence to predict a single value (generate text: next word)*\n",
        "* **Vector-to-Sequence** (One to Many)\n",
        "\n",
        "## Issues with RNNs\n",
        "Main disadvantage is that RNN only really remember the previous output. It would be useful if we could keep track of longer history, not just short term history.\n",
        "\n",
        "\n",
        "**1. Vanishing Gradient** as you go back to the 'lower' layers, gradients often get smaller, eventually causing weights to never change at lower layers.\n",
        "\n",
        "**2. Exploding Gradient** the opposite to 1., gradients 'explode' on the way back, causing issues.\n",
        "\n",
        "**Solution 1**:\n",
        "* Using ReLU\n",
        "* Uning 'Leaky' ReLU\n",
        "* Exponential Linear Unit (ELU)\n",
        "\n",
        "**Solution 2**:\n",
        "* **Batch normalization** - model will normalize each batch using the batch mean and standard deviation.\n",
        "\n",
        "* **Xavier Initialization** - choosing different initialization of weights\n",
        "\n",
        "* **Gradient Clipping** (for exploding gradient) - a 'quick fix': gradients are cut off before reaching a predetermined limit (cut off the gradient to be between -1 and 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXz64aSIsBBm"
      },
      "source": [
        "## LSTM (Long Short-Term Memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V35OMy57sL-C"
      },
      "source": [
        "The solutions specified above could slow down training.\n",
        "\n",
        "After a while the network will begin to 'forget' the first inputs, as information is lost at each step going through RNN.\n",
        "\n",
        "So we need \"long-term\" memory.\n",
        "\n",
        "To address this we have **LSTM(Long Short-Term Memory)**\n",
        "\n",
        "Structure of LSTM:\n",
        "* **Forget Gate** - decides what to forget from the previous memory units.\n",
        "* **Input Gate** - decides what to accept into a neuron.\n",
        "* **Output Gate** - outputs the new long-term memory. \n",
        "* **Update Gate** - updates the memory.\n",
        "\n",
        "\n",
        "**First step** is to decide what information are we going to throw away/forget from the cell state (pass the weights through sigmoid function: the closer it is to 0, the more are the chances it will be thrown away, the closer it is to 1 - the more chances to keep)\n",
        "\n",
        "**Second step** is to decide what new information are we going to store to the cell state. 2 parts:\n",
        "\n",
        "* Input gate layer\n",
        "* Hyperbolic tangent layer, creating a vector of new candidates.\n",
        "\n",
        "**Third step** is to update the old cell state: multiply the old state by f(t) forgetting the info we've decided are not important, then we add candidates\n",
        "\n",
        "**Fourth step** is to decide what we are going to output. So we run a sigmoid layer to decide what parts of the cell state we're going to output. Then we put that cell state through a hyperbolic tangent function which pushes all the values to be between -1 and 1. And than we multiply it by the output of the initial sigmoid gate. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxosYOIB-_UC"
      },
      "source": [
        "## Gated Recurrent Unit (GRU)\n",
        "\n",
        "A variation of LSTM which combines the forget and input gates into 'single update' gate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55BgY7EX_U7q"
      },
      "source": [
        "## Basic RNN"
      ]
    }
  ]
}